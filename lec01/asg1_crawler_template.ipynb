{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新浪股票公司公告爬取作业说明\n",
    "任务目标：\n",
    "- 选取新浪股票某一家公司，爬取它的100条**不重复**的企业公告，以csv文件存储。（70，“重复”-10）【beautiful soup和re正则表达式二选一】\n",
    "- csv数据列包含公告【标题】、【日期】、【正文】。（+10）\n",
    "- 加分：按照ipynb中的提示，完成比较html解析方法效率的小实验。（+10）\n",
    "- 加分：将其分类【类别】属性也作为一列存储。（+10）\n",
    "\n",
    "# 作业总体思路\n",
    "总体思路：\n",
    "1. 获取公告列表；\n",
    "2. 获取文本内容；\n",
    "3. 数据存储；\n",
    "4. （加分）公告的“分类类别”属性；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取公告列表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分析网页链接（接口）和参数\n",
    "\n",
    "- 首次打开一个公司的公告页，它的链接是？\n",
    "- 点击”第二页”，检查公告页链接，它的组成部分是？\n",
    "- 参数是如何通过接口输入的？\n",
    "\n",
    "通过上述尝试，构建一个可以用来请求【某公司】【某页】公告页的网页链接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 填充下列变量的赋值号右边\n",
    "pageno=\n",
    "stockid=\n",
    "url=\n",
    "# 提示：使用str.format将参数引入字符串"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 请求和解析列表页，获得网页链接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 编写你的第一个`requests.get()`函数\n",
    "\n",
    "- 参考ppt材料，可以学习requests中的其他参数的用法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_text(url:str)-> str:\n",
    "    \"\"\"\n",
    "    描述：构造这个函数，输入一个链接，使用requests爬取该链接，\n",
    "    如果其响应代码为[200]，则返回其html的字符串。（返回值类型为str）\n",
    "    这个函数可以在之后的代码中反复使用！\n",
    "    \"\"\"\n",
    "    # 将如下修改为你的代码\n",
    "    text='html text'\n",
    "    \n",
    "    # 填充完毕\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解析列表页（两种方法二选一）\n",
    "\n",
    "使用任意一种方法解析列表页，将公告列表页的信息存储为一个dataframe，【至少】包括如下数据列：\n",
    "```python\n",
    "cols=['title','date','url']\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### beautifulsoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame(columns=['title','date','url'])\n",
    "\n",
    "# 请补完后续代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### re正则表达式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame(columns=['title','date','url'])\n",
    "\n",
    "# 请补完后续代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小实验：比较不同方法的解析提取效率（+10）\n",
    "\n",
    "- （可选：学习lxml库的xml.etree()的用法。xml是beautifulsoup的依赖库。尝试用etree对象的xpath语法而不是beautifulsoup.find()函数来获取你想要的属性）\n",
    "- （在本实验中，希望你获取公告列表上方【全文类型】字段中的所有全文类型的标识代码-类型名称）\n",
    "- 尝试使用正则表达式提取该信息\n",
    "- 循环100次，比较至少两种方法（re vs bs, bs vs etree, re vs etree）的运行效率。直接输出两个运行时间即可。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![所需字段的页面显示](https://img1.imgtp.com/2022/09/24/l5f68NEI.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里调用了最开始设计的函数\n",
    "html_text=get_html_text(url)\n",
    "category_dict={\n",
    "    '0':'全部', # 需要从页面上解析获取数据，以填充该字典！\n",
    "}\n",
    "\n",
    "# 请修改和完成如下函数，完成两种不同的解析方式\n",
    "def parse_method1(text,category_dict):\n",
    "    \"\"\"\n",
    "    你的方法一\n",
    "    \"\"\"\n",
    "\n",
    "    return category_dict\n",
    "\n",
    "def parse_method2(text,category_dict):\n",
    "    \"\"\"\n",
    "    你的方法二\n",
    "    \"\"\"\n",
    "\n",
    "    return category_dict\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "def compare():\n",
    "    \"\"\"\n",
    "    比较时间效率。该函数仅供参考。可以自行编写。\n",
    "    \"\"\"\n",
    "    iteration=100\n",
    "\n",
    "    m1_begin=time.time()\n",
    "    for i in range(iteration):\n",
    "        text=html_text\n",
    "        parse_method1(text,category_dict)\n",
    "    m1_end=time.time()\n",
    "\n",
    "    m2_begin=time.time()\n",
    "    for i in range(iteration):\n",
    "        text=html_text\n",
    "        parse_method2(text,category_dict)\n",
    "    m2_end=time.time()\n",
    "\n",
    "    print(\"m1 time cost:\",m1_end-m1_begin)\n",
    "    print(\"m1 time cost:\",m2_end-m2_begin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 爬取公告文本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解析公告文本（二选一）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### beautiful soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_content(url:str) -> str:\n",
    "    content=\"\"\n",
    "    # 完成下面部分的代码\n",
    "\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### re正则表达式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_content(url:str) -> str:\n",
    "    content=\"\"\n",
    "    # 完成下面部分的代码\n",
    "\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 遍历公告链接，填充数据列并保存数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此处使用了DataFrame的apply函数。你也可以修改为你喜欢的遍历调用方法。\n",
    "# 我认为apply函数用于遍历链接和获取数据常常不是最好用的。为什么？\n",
    "# 因为当apply函数因为报错中断时，之前遍历得到的数据都得不到保存。\n",
    "# 而在爬虫实践中，报错才是常态……\n",
    "\n",
    "\n",
    "# 以下代码可以任意修改。命名格式参照如下。\n",
    "\n",
    "data['content']=data['url'].apply(parse_content)\n",
    "\n",
    "your_name=\"\"\n",
    "your_student_no=\"\"\n",
    "data.to_csv(f'./homework1_{your_name}_{your_student_no}_stockid{stockid}_{len(data)}条.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "731.884px",
    "left": "772.85px",
    "right": "20px",
    "top": "67.9583px",
    "width": "596.004px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
