{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Topic Modeling using Scikit-Learn\n",
    "\n",
    "As for the dataset, you can choose to use your own or download the publicly available [20 News Group Dataset](http://qwone.com/~jason/20Newsgroups/). It consists of approximately 20k documents related to newsgroup. \n",
    "\n",
    "There are altogether 3 variations:\n",
    "* [20news-19997.tar.gz](http://qwone.com/~jason/20Newsgroups/20news-19997.tar.gz) — contains the original unmodified 20 Newsgroups data set\n",
    "* [20news-bydate.tar.gz](http://qwone.com/~jason/20Newsgroups/20news-bydate.tar.gz) — dataset is sorted by date in addition to the removal of duplicates and some headers. Split into train and test folder.\n",
    "* [20news-18828.tar.gz](http://qwone.com/~jason/20Newsgroups/20news-18828.tar.gz) — duplicates are removed and headers contain only From and Subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using the `20news-18828` dataset in this tutorial. \n",
    "To keep things simple and short, I am going to use only **5 topics out of 20**:\n",
    "`['rec.sport.hockey', 'soc.religion.christian', 'talk.politics.mideast', 'comp.graphics', 'sci.crypt']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理\n",
    "* 首先将`20news-18828.tar.gz`解压缩\n",
    "* 运行下面的代码，将`base_topics`相关的文本放到一个文件`data.json`中\n",
    "* 后续你也可以尝试更多的类别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "\n",
    "base_topics = ['rec.sport.hockey', 'soc.religion.christian', 'talk.politics.mideast', 'comp.graphics', 'sci.crypt']\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for i in base_topics:\n",
    "    for j in glob.glob(f'./20news-18828/{i}/*'):\n",
    "        with open(j, 'r', encoding='cp1252') as f:\n",
    "            data.append(f.read())\n",
    "            labels.append(i)\n",
    "\n",
    "with open('data.json', 'w', encoding='utf8') as f:\n",
    "    f.write(json.dumps(data))\n",
    "with open('label.json','w', encoding='utf8') as f:\n",
    "    f.write(json.dumps(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入相关的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "import numpy as np\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从文件载入数据集\n",
    "* 载入之后，一般会对数据做一次`shuffle`操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"From: bc744@cleveland.Freenet.Edu (Mark Ira Kaufman)\\nSubject: About this 'Center for Policy Research'...\\n\\n\\n   I have read numerous posts over a period of several months, by\\nthis anti-Israel fanatic, hiding in the shadow of the respectable\\nsounding name of the 'Center for Policy Research.'  Obviously, it\\nis no research center of any kind, unless 'researching' published\\ndocuments to find material to use against Israel makes it so.  \\n\\n   Labeling a propaganda mill a research center is not surprising\\nin itself.  That is simply part of the propaganda process.  I was\\ncurious if anyone knew who this anti-Israel fanatic hiding behind\\nhis phoney 'research center' name is.  Is he an Arab?  Is he some\\ntypical anti-semite hiding behind a veneer of 'anti-zionism?'  Is\\nhe some Jew who perhaps lived in Israel and just couldn't make it\\nthere, and is now taking his failure out on Israel?  \\n\\n   Let's shed some light on this clown once and for all.  It will\\nhelp put his nonsense in the proper perspective.  And the readers\\nof this group who are more interested in fact than in anti-Israel\\nhyperbola can ignore this junk.\\n\\n\"]\n",
      "['talk.politics.mideast']\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "with open('data.json', 'r', encoding='utf8') as f:\n",
    "    X = json.loads(f.read())\n",
    "with open('label.json', 'r', encoding='utf8') as f:\n",
    "    y = json.loads(f.read())\n",
    "    \n",
    "randnum = random.randint(0,100)\n",
    "random.seed(randnum)\n",
    "random.shuffle(X)\n",
    "random.seed(randnum)\n",
    "random.shuffle(y)\n",
    "print(X[:1])\n",
    "print(y[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 划分训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # function for splitting data to train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成文本的独热表达\n",
    "* `CountVectorizer` converts a collection of text documents to a matrix which contains all the token counts. Sometimes, token count is referred to as term frequency.\n",
    "* Unlike `CountVectorizer`, `TfidfVectorizer` converts documents to a matrix of TF-IDF features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 限制作为特征的单词个数\n",
    "n_features = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>128</th>\n",
       "      <th>13</th>\n",
       "      <th>...</th>\n",
       "      <th>writing</th>\n",
       "      <th>written</th>\n",
       "      <th>wrong</th>\n",
       "      <th>wrote</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>years ago</th>\n",
       "      <th>yes</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  01  02  10  100  11  12  128  13  ...  writing  written  wrong  \\\n",
       "0   0    0   0   0   0    0   0   0    0   0  ...        0        0      1   \n",
       "1   0    0   0   0   0    0   0   0    0   0  ...        0        0      1   \n",
       "2   0    0   0   0   0    0   0   0    0   0  ...        0        0      0   \n",
       "3   0    1   0   0   0    0   0   0    0   0  ...        0        0      0   \n",
       "4   0    0   0   0   0    0   0   0    0   0  ...        0        0      0   \n",
       "\n",
       "   wrote  year  years  years ago  yes  york  young  \n",
       "0      0     2      0          0    0     0      0  \n",
       "1      0     0      0          0    0     0      0  \n",
       "2      0     1      0          0    0     0      0  \n",
       "3      0     0      0          0    0     0      0  \n",
       "4      0     1      0          0    0     0      0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=n_features, stop_words='english', ngram_range=(1, 2))\n",
    "tf = tf_vectorizer.fit_transform(X_train)\n",
    "features = pd.DataFrame(tf.toarray(), columns=tf_vectorizer.get_feature_names())\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>128</th>\n",
       "      <th>13</th>\n",
       "      <th>...</th>\n",
       "      <th>writing</th>\n",
       "      <th>written</th>\n",
       "      <th>wrong</th>\n",
       "      <th>wrote</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>years ago</th>\n",
       "      <th>yes</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.065268</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.123587</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.136380</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.141576</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.158867</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.123731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00       000   01   02   10  100   11   12  128   13  ...  writing  \\\n",
       "0  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...      0.0   \n",
       "1  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...      0.0   \n",
       "2  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...      0.0   \n",
       "3  0.0  0.158867  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...      0.0   \n",
       "4  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...      0.0   \n",
       "\n",
       "   written     wrong  wrote      year  years  years ago  yes  york  young  \n",
       "0      0.0  0.065268    0.0  0.123587    0.0        0.0  0.0   0.0    0.0  \n",
       "1      0.0  0.136380    0.0  0.000000    0.0        0.0  0.0   0.0    0.0  \n",
       "2      0.0  0.000000    0.0  0.141576    0.0        0.0  0.0   0.0    0.0  \n",
       "3      0.0  0.000000    0.0  0.000000    0.0        0.0  0.0   0.0    0.0  \n",
       "4      0.0  0.000000    0.0  0.123731    0.0        0.0  0.0   0.0    0.0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, \n",
    "                                   max_features=n_features, stop_words='english', ngram_range=(1, 2))\n",
    "tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "features = pd.DataFrame(tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用朴素贝叶斯分类器分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集上其他指标：\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "         comp.graphics       0.90      0.96      0.93       204\n",
      "      rec.sport.hockey       0.97      0.96      0.97       196\n",
      "             sci.crypt       0.95      0.94      0.94       188\n",
      "soc.religion.christian       0.96      0.90      0.93       193\n",
      " talk.politics.mideast       0.93      0.93      0.93       199\n",
      "\n",
      "              accuracy                           0.94       980\n",
      "             macro avg       0.94      0.94      0.94       980\n",
      "          weighted avg       0.94      0.94      0.94       980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(tf, y_train)\n",
    "X_test_tf = tf_vectorizer.transform(X_test)\n",
    "y_predict = nb.predict(X_test_tf)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"测试集上其他指标：\\n\",classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集上其他指标：\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "         comp.graphics       0.87      0.96      0.91       204\n",
      "      rec.sport.hockey       0.96      0.97      0.97       196\n",
      "             sci.crypt       0.96      0.93      0.95       188\n",
      "soc.religion.christian       0.97      0.91      0.94       193\n",
      " talk.politics.mideast       0.96      0.95      0.95       199\n",
      "\n",
      "              accuracy                           0.94       980\n",
      "             macro avg       0.95      0.94      0.94       980\n",
      "          weighted avg       0.94      0.94      0.94       980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb.fit(tfidf, y_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "y_predict = nb.predict(X_test_tfidf)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"测试集上其他指标：\\n\",classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成文本的主题模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 1000\n",
    "n_components = 5\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_topics = ['soc.religion.christian', 'sci.crypt', 'rec.sport.hockey', 'talk.politics.mideast', 'comp.graphics']\n",
    "lda_topics = ['talk.politics.mideast', 'rec.sport.hockey', 'soc.religion.christian', 'sci.crypt', 'comp.graphics']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis\n",
    "`scikit-learn` also comes with a great and useful dimensionality reduction model called `Truncated Singular Value Decomposition` (`TruncatedSVD`). \n",
    "\n",
    "In the event where `TruncatedSVD` model is fitted with `count` or `tfidf` matrices, it is also known as `Latent Semantic Analysis (LSA)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa = TruncatedSVD(n_components=n_components, random_state=1, algorithm='arpack').fit(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LatentDirichletAllocation (LDA)\n",
    "LDA is a good generative probabilistic model for identifying abstract topics from discrete dataset such as text corpora.\n",
    "\n",
    "\n",
    "You should use CountVectorizer when fitting LDA instead of TfidfVectorizer since LDA is based on term count and document count. Fitting LDA with TfidfVectorizer will result in rare words being dis-proportionally sampled. As a result, they will have greater impact and influence on the final topic distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=n_components, random_state=1).fit(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看学习到的Topics信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_topics(model, vectorizer, topics, n_top_words=n_top_words):\n",
    "    word_dict = {}\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        word_dict[topics[topic_idx]] = top_features\n",
    "\n",
    "    return pd.DataFrame(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>soc.religion.christian</th>\n",
       "      <th>sci.crypt</th>\n",
       "      <th>rec.sport.hockey</th>\n",
       "      <th>talk.politics.mideast</th>\n",
       "      <th>comp.graphics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>edu</td>\n",
       "      <td>key</td>\n",
       "      <td>game</td>\n",
       "      <td>israel</td>\n",
       "      <td>graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>com</td>\n",
       "      <td>clipper</td>\n",
       "      <td>team</td>\n",
       "      <td>israeli</td>\n",
       "      <td>edu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>god</td>\n",
       "      <td>chip</td>\n",
       "      <td>hockey</td>\n",
       "      <td>jews</td>\n",
       "      <td>image</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>writes</td>\n",
       "      <td>encryption</td>\n",
       "      <td>ca</td>\n",
       "      <td>armenian</td>\n",
       "      <td>thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>people</td>\n",
       "      <td>com</td>\n",
       "      <td>espn</td>\n",
       "      <td>turkish</td>\n",
       "      <td>file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>article</td>\n",
       "      <td>keys</td>\n",
       "      <td>games</td>\n",
       "      <td>armenians</td>\n",
       "      <td>files</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>don</td>\n",
       "      <td>escrow</td>\n",
       "      <td>nhl</td>\n",
       "      <td>arab</td>\n",
       "      <td>program</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>just</td>\n",
       "      <td>government</td>\n",
       "      <td>season</td>\n",
       "      <td>armenia</td>\n",
       "      <td>ac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>know</td>\n",
       "      <td>clipper chip</td>\n",
       "      <td>players</td>\n",
       "      <td>jewish</td>\n",
       "      <td>3d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>like</td>\n",
       "      <td>netcom</td>\n",
       "      <td>play</td>\n",
       "      <td>arabs</td>\n",
       "      <td>gif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>think</td>\n",
       "      <td>netcom com</td>\n",
       "      <td>year</td>\n",
       "      <td>turks</td>\n",
       "      <td>format</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>does</td>\n",
       "      <td>algorithm</td>\n",
       "      <td>edu</td>\n",
       "      <td>war</td>\n",
       "      <td>ftp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ca</td>\n",
       "      <td>nsa</td>\n",
       "      <td>leafs</td>\n",
       "      <td>turkey</td>\n",
       "      <td>looking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>key</td>\n",
       "      <td>crypto</td>\n",
       "      <td>pens</td>\n",
       "      <td>jake</td>\n",
       "      <td>help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>good</td>\n",
       "      <td>phone</td>\n",
       "      <td>player</td>\n",
       "      <td>muslim</td>\n",
       "      <td>images</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>time</td>\n",
       "      <td>use</td>\n",
       "      <td>wings</td>\n",
       "      <td>org</td>\n",
       "      <td>uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>clipper</td>\n",
       "      <td>security</td>\n",
       "      <td>fans</td>\n",
       "      <td>killed</td>\n",
       "      <td>mail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>israel</td>\n",
       "      <td>des</td>\n",
       "      <td>buffalo</td>\n",
       "      <td>serdar</td>\n",
       "      <td>package</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>cs</td>\n",
       "      <td>sternlight</td>\n",
       "      <td>win</td>\n",
       "      <td>peace</td>\n",
       "      <td>windows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>say</td>\n",
       "      <td>public</td>\n",
       "      <td>cup</td>\n",
       "      <td>argic</td>\n",
       "      <td>software</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   soc.religion.christian     sci.crypt rec.sport.hockey  \\\n",
       "0                     edu           key             game   \n",
       "1                     com       clipper             team   \n",
       "2                     god          chip           hockey   \n",
       "3                  writes    encryption               ca   \n",
       "4                  people           com             espn   \n",
       "5                 article          keys            games   \n",
       "6                     don        escrow              nhl   \n",
       "7                    just    government           season   \n",
       "8                    know  clipper chip          players   \n",
       "9                    like        netcom             play   \n",
       "10                  think    netcom com             year   \n",
       "11                   does     algorithm              edu   \n",
       "12                     ca           nsa            leafs   \n",
       "13                    key        crypto             pens   \n",
       "14                   good         phone           player   \n",
       "15                   time           use            wings   \n",
       "16                clipper      security             fans   \n",
       "17                 israel           des          buffalo   \n",
       "18                     cs    sternlight              win   \n",
       "19                    say        public              cup   \n",
       "\n",
       "   talk.politics.mideast comp.graphics  \n",
       "0                 israel      graphics  \n",
       "1                israeli           edu  \n",
       "2                   jews         image  \n",
       "3               armenian        thanks  \n",
       "4                turkish          file  \n",
       "5              armenians         files  \n",
       "6                   arab       program  \n",
       "7                armenia            ac  \n",
       "8                 jewish            3d  \n",
       "9                  arabs           gif  \n",
       "10                 turks        format  \n",
       "11                   war           ftp  \n",
       "12                turkey       looking  \n",
       "13                  jake          help  \n",
       "14                muslim        images  \n",
       "15                   org            uk  \n",
       "16                killed          mail  \n",
       "17                serdar       package  \n",
       "18                 peace       windows  \n",
       "19                 argic      software  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_topics(lsa, tfidf_vectorizer, lsa_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>talk.politics.mideast</th>\n",
       "      <th>rec.sport.hockey</th>\n",
       "      <th>soc.religion.christian</th>\n",
       "      <th>sci.crypt</th>\n",
       "      <th>comp.graphics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>edu</td>\n",
       "      <td>god</td>\n",
       "      <td>edu</td>\n",
       "      <td>edu</td>\n",
       "      <td>turkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>com</td>\n",
       "      <td>edu</td>\n",
       "      <td>people</td>\n",
       "      <td>image</td>\n",
       "      <td>armenian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>key</td>\n",
       "      <td>people</td>\n",
       "      <td>israel</td>\n",
       "      <td>graphics</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>writes</td>\n",
       "      <td>jesus</td>\n",
       "      <td>writes</td>\n",
       "      <td>file</td>\n",
       "      <td>armenians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>game</td>\n",
       "      <td>church</td>\n",
       "      <td>jews</td>\n",
       "      <td>jpeg</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>clipper</td>\n",
       "      <td>think</td>\n",
       "      <td>article</td>\n",
       "      <td>use</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>encryption</td>\n",
       "      <td>christ</td>\n",
       "      <td>said</td>\n",
       "      <td>mail</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>article</td>\n",
       "      <td>believe</td>\n",
       "      <td>israeli</td>\n",
       "      <td>com</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>chip</td>\n",
       "      <td>know</td>\n",
       "      <td>just</td>\n",
       "      <td>available</td>\n",
       "      <td>armenia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>team</td>\n",
       "      <td>christian</td>\n",
       "      <td>like</td>\n",
       "      <td>files</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ca</td>\n",
       "      <td>does</td>\n",
       "      <td>know</td>\n",
       "      <td>data</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>don</td>\n",
       "      <td>christians</td>\n",
       "      <td>don</td>\n",
       "      <td>software</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hockey</td>\n",
       "      <td>db</td>\n",
       "      <td>jewish</td>\n",
       "      <td>bit</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>just</td>\n",
       "      <td>don</td>\n",
       "      <td>did</td>\n",
       "      <td>ftp</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>think</td>\n",
       "      <td>writes</td>\n",
       "      <td>think</td>\n",
       "      <td>information</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>government</td>\n",
       "      <td>just</td>\n",
       "      <td>right</td>\n",
       "      <td>images</td>\n",
       "      <td>turks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>like</td>\n",
       "      <td>say</td>\n",
       "      <td>war</td>\n",
       "      <td>program</td>\n",
       "      <td>turkey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>good</td>\n",
       "      <td>bible</td>\n",
       "      <td>arab</td>\n",
       "      <td>computer</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>new</td>\n",
       "      <td>like</td>\n",
       "      <td>time</td>\n",
       "      <td>like</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>time</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>gif</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   talk.politics.mideast rec.sport.hockey soc.religion.christian    sci.crypt  \\\n",
       "0                    edu              god                    edu          edu   \n",
       "1                    com              edu                 people        image   \n",
       "2                    key           people                 israel     graphics   \n",
       "3                 writes            jesus                 writes         file   \n",
       "4                   game           church                   jews         jpeg   \n",
       "5                clipper            think                article          use   \n",
       "6             encryption           christ                   said         mail   \n",
       "7                article          believe                israeli          com   \n",
       "8                   chip             know                   just    available   \n",
       "9                   team        christian                   like        files   \n",
       "10                    ca             does                   know         data   \n",
       "11                   don       christians                    don     software   \n",
       "12                hockey               db                 jewish          bit   \n",
       "13                  just              don                    did          ftp   \n",
       "14                 think           writes                  think  information   \n",
       "15            government             just                  right       images   \n",
       "16                  like              say                    war      program   \n",
       "17                  good            bible                   arab     computer   \n",
       "18                   new             like                   time         like   \n",
       "19                  time              com                    com          gif   \n",
       "\n",
       "   comp.graphics  \n",
       "0        turkish  \n",
       "1       armenian  \n",
       "2             10  \n",
       "3      armenians  \n",
       "4             25  \n",
       "5             11  \n",
       "6             12  \n",
       "7             14  \n",
       "8        armenia  \n",
       "9             20  \n",
       "10            15  \n",
       "11            16  \n",
       "12            55  \n",
       "13            18  \n",
       "14            17  \n",
       "15         turks  \n",
       "16        turkey  \n",
       "17            13  \n",
       "18            30  \n",
       "19            19  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_topics(lda, tf_vectorizer, lda_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于学习到的模型进行推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inference(model, vectorizer, topics, text, threshold):\n",
    "    v_text = vectorizer.transform([text])\n",
    "    score = model.transform(v_text)\n",
    "    labels = set()\n",
    "    for i in range(len(score[0])):\n",
    "        if score[0][i] > threshold:\n",
    "            labels.add(topics[i])\n",
    "\n",
    "    if not labels:\n",
    "        return 'None', -1, set()\n",
    "\n",
    "    return topics[np.argmax(score)], score, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.graphics\n",
      "[[ 0.05302177  0.05875669  0.00225079 -0.03293454  0.14861712]]\n"
     ]
    }
   ],
   "source": [
    "text = 'you should use either jpeg or png files for it'\n",
    "topic, score, labels = get_inference(lsa, tfidf_vectorizer, lsa_topics, text, 0)\n",
    "print(topic)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sci.crypt\n",
      "[[0.05027019 0.05013133 0.05006827 0.7995194  0.05001081]]\n"
     ]
    }
   ],
   "source": [
    "topic, score, labels = get_inference(lda, tf_vectorizer, lda_topics, text, 0)\n",
    "print(topic)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用学习到的特征分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集上其他指标：\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "         comp.graphics       0.88      0.89      0.89       204\n",
      "      rec.sport.hockey       0.61      0.81      0.70       196\n",
      "             sci.crypt       0.65      0.42      0.51       188\n",
      "soc.religion.christian       0.91      0.88      0.90       193\n",
      " talk.politics.mideast       0.88      0.92      0.90       199\n",
      "\n",
      "              accuracy                           0.79       980\n",
      "             macro avg       0.79      0.78      0.78       980\n",
      "          weighted avg       0.79      0.79      0.78       980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda_vec = lda.transform(tf)\n",
    "nb.fit(lda_vec, y_train)\n",
    "\n",
    "lda_test_vec = lda.transform(X_test_tf)\n",
    "y_predict = nb.predict(lda_test_vec)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"测试集上其他指标：\\n\",classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 请针对上述实验结果，分析LDA模型的适用性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
