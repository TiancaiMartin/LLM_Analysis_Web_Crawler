{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Topic Modeling using Scikit-Learn\n",
    "\n",
    "As for the dataset, you can choose to use your own or download the publicly available [20 News Group Dataset](http://qwone.com/~jason/20Newsgroups/). It consists of approximately 20k documents related to newsgroup. \n",
    "\n",
    "There are altogether 3 variations:\n",
    "* [20news-19997.tar.gz](http://qwone.com/~jason/20Newsgroups/20news-19997.tar.gz) — contains the original unmodified 20 Newsgroups data set\n",
    "* [20news-bydate.tar.gz](http://qwone.com/~jason/20Newsgroups/20news-bydate.tar.gz) — dataset is sorted by date in addition to the removal of duplicates and some headers. Split into train and test folder.\n",
    "* [20news-18828.tar.gz](http://qwone.com/~jason/20Newsgroups/20news-18828.tar.gz) — duplicates are removed and headers contain only From and Subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using the `20news-18828` dataset in this tutorial. \n",
    "To keep things simple and short, I am going to use only **5 topics out of 20**:\n",
    "`['rec.sport.hockey', 'soc.religion.christian', 'talk.politics.mideast', 'comp.graphics', 'sci.crypt']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理\n",
    "* 首先将`20news-18828.tar.gz`解压缩\n",
    "* 运行下面的代码，将`base_topics`相关的文本放到一个文件`data.json`中\n",
    "* 后续你也可以尝试更多的类别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "\n",
    "# 选用的5个topics\n",
    "base_topics = ['rec.sport.hockey', 'soc.religion.christian', 'talk.politics.mideast', 'comp.graphics', 'sci.crypt']\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for i in base_topics:\n",
    "    for j in glob.glob(f'./20news-18828/{i}/*'):  #获取这个路径下的所有文件\n",
    "        with open(j, 'r', encoding='cp1252') as f: #cp1252是MS Windows英文版安装的默认编码\n",
    "            data.append(f.read())\n",
    "            labels.append(i)\n",
    "# json.dumps() 是把python对象转换成json对象的一个过程，生成的是字符串\n",
    "with open('data.json', 'w', encoding='utf8') as f: \n",
    "    f.write(json.dumps(data))\n",
    "with open('label.json','w', encoding='utf8') as f:\n",
    "    f.write(json.dumps(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入相关的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# 两个主题分析模型：LDA模型,基于截断的SVD模型\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "import numpy as np\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从文件载入数据集\n",
    "* 载入之后，一般会对数据做一次`shuffle`操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"From: shadow@r-node.hub.org (Jay Chu)\\nSubject: Lindros will be traded!!!\\n\\nTrue rumor.  Fact!  A big three way deal!\\n\\nEric Lindros going to Ottawa Senators.  And Senators get $15mill from\\nMontreal.\\n\\nMontreal gets Alexander Daigle (the first round pick from Senators)\\n\\nPhilly gets Damphousse, Bellow, Patrick Roy and a draft pick.\\n\\n-- \\n        ______                shadow@r-node.gts.org\\n       | |__| |   If it's there and you can see it       - it's real\\n       |  ()  |   If it's there and you can't see it     - it's transparent\\n       |______|   If it's not there and you can't see it - you erased it!\\n\"]\n",
      "['rec.sport.hockey']\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "# 读入数据\n",
    "with open('data.json', 'r', encoding='utf8') as f:\n",
    "    X = json.loads(f.read())\n",
    "with open('label.json', 'r', encoding='utf8') as f:\n",
    "    y = json.loads(f.read())\n",
    "\n",
    "# 打乱数据\n",
    "randnum = random.randint(0,100)\n",
    "random.seed(randnum)\n",
    "random.shuffle(X)\n",
    "random.seed(randnum)\n",
    "random.shuffle(y)\n",
    "print(X[:1])\n",
    "print(y[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 划分训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # function for splitting data to train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成文本的独热表达\n",
    "* `CountVectorizer` converts a collection of text documents to a matrix which contains all the token counts. Sometimes, token count is referred to as term frequency.\n",
    "* Unlike `CountVectorizer`, `TfidfVectorizer` converts documents to a matrix of TF-IDF features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CountVectornizer:将文本中的词语转换为词频矩阵\n",
    "- TfidVectorizer:将文本的词语转换为对应的TF-IDF矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 限制作为特征的单词个数\n",
    "n_features = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 参数\n",
    "    - max_df:可以设置为范围在[0.0 1.0]的float，也可以设置为没有范围限制的int，默认为1.0。这个参数的作用是作为一个阈值，当构造语料库的关键词集的时候，如果某个词的document frequence大于max_df，这个词不会被当作关键词。如果这个参数是float，则表示词出现的次数与语料库文档数的百分比，如果是int，则表示词出现的次数。如果参数中已经给定了vocabulary，则这个参数无效\n",
    "    - min_df:类似于max_df，不同之处在于如果某个词的document frequence小于min_df，则这个词不会被当作关键词\n",
    "    - ngram_range=(1,2):表示选用1到2个词进行前后的组合，构成新的标签值,比如“I love you”——\"I love\" \"love you\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>02</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>128</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>...</th>\n",
       "      <th>writes article</th>\n",
       "      <th>written</th>\n",
       "      <th>wrong</th>\n",
       "      <th>wrote</th>\n",
       "      <th>xv</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yes</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  02  10  100  11  12  128  13  14  ...  writes article  written  \\\n",
       "0   0    0   0   0    0   0   0    0   0   0  ...               0        0   \n",
       "1   0    0   0   0    0   0   0    0   0   0  ...               0        0   \n",
       "2   0    0   0   0    0   0   1    0   0   0  ...               1        0   \n",
       "3   0    0   0   0    0   0   0    0   0   0  ...               0        0   \n",
       "4   0    1   0   0    0   0   0    0   0   0  ...               1        0   \n",
       "\n",
       "   wrong  wrote  xv  year  years  yes  york  young  \n",
       "0      1      0   0     0      0    0     0      0  \n",
       "1      1      0   0     0      0    0     0      0  \n",
       "2      0      0   0     0      0    0     0      0  \n",
       "3      0      0   0     0      1    0     0      0  \n",
       "4      0      0   0     0      0    0     0      0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 利用CountVectorizer将文本表示成向量的形式\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=n_features, stop_words='english', ngram_range=(1, 2))\n",
    "tf = tf_vectorizer.fit_transform(X_train)\n",
    "features = pd.DataFrame(tf.toarray(), columns=tf_vectorizer.get_feature_names())\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>02</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>128</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>...</th>\n",
       "      <th>writes article</th>\n",
       "      <th>written</th>\n",
       "      <th>wrong</th>\n",
       "      <th>wrote</th>\n",
       "      <th>xv</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yes</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.147727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.131053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050566</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046590</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.061037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048738</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039226</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00       000   02   10  100   11        12  128   13   14  ...  \\\n",
       "0  0.0  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  ...   \n",
       "1  0.0  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  ...   \n",
       "2  0.0  0.000000  0.0  0.0  0.0  0.0  0.050566  0.0  0.0  0.0  ...   \n",
       "3  0.0  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  ...   \n",
       "4  0.0  0.048738  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  ...   \n",
       "\n",
       "   writes article  written     wrong  wrote   xv  year     years  yes  york  \\\n",
       "0        0.000000      0.0  0.147727    0.0  0.0   0.0  0.000000  0.0   0.0   \n",
       "1        0.000000      0.0  0.131053    0.0  0.0   0.0  0.000000  0.0   0.0   \n",
       "2        0.046590      0.0  0.000000    0.0  0.0   0.0  0.000000  0.0   0.0   \n",
       "3        0.000000      0.0  0.000000    0.0  0.0   0.0  0.061037  0.0   0.0   \n",
       "4        0.039226      0.0  0.000000    0.0  0.0   0.0  0.000000  0.0   0.0   \n",
       "\n",
       "   young  \n",
       "0    0.0  \n",
       "1    0.0  \n",
       "2    0.0  \n",
       "3    0.0  \n",
       "4    0.0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 利用TfidfVectorizer将文本表示成向量的形式\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, \n",
    "                                   max_features=n_features, stop_words='english', ngram_range=(1, 2))\n",
    "tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "features = pd.DataFrame(tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用朴素贝叶斯分类器分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集上其他指标：\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "         comp.graphics       0.93      0.92      0.93       199\n",
      "      rec.sport.hockey       0.98      0.98      0.98       197\n",
      "             sci.crypt       0.93      0.94      0.93       198\n",
      "soc.religion.christian       0.94      0.95      0.94       196\n",
      " talk.politics.mideast       0.94      0.92      0.93       190\n",
      "\n",
      "              accuracy                           0.94       980\n",
      "             macro avg       0.94      0.94      0.94       980\n",
      "          weighted avg       0.94      0.94      0.94       980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB() # 构建一个分类器\n",
    "nb.fit(tf, y_train) #用训练集训练分类器\n",
    "X_test_tf = tf_vectorizer.transform(X_test) #embedding测试集\n",
    "y_predict = nb.predict(X_test_tf) # 利用测试集去做预测\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"测试集上其他指标：\\n\",classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集上其他指标：\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "         comp.graphics       0.93      0.94      0.93       199\n",
      "      rec.sport.hockey       0.98      0.98      0.98       197\n",
      "             sci.crypt       0.93      0.96      0.95       198\n",
      "soc.religion.christian       0.96      0.95      0.96       196\n",
      " talk.politics.mideast       0.98      0.94      0.96       190\n",
      "\n",
      "              accuracy                           0.96       980\n",
      "             macro avg       0.96      0.96      0.96       980\n",
      "          weighted avg       0.96      0.96      0.96       980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb.fit(tfidf, y_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "y_predict = nb.predict(X_test_tfidf)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"测试集上其他指标：\\n\",classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成文本的主题模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 1000\n",
    "n_components = 5\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_topics = ['soc.religion.christian', 'sci.crypt', 'rec.sport.hockey', 'talk.politics.mideast', 'comp.graphics']\n",
    "lda_topics = ['talk.politics.mideast', 'rec.sport.hockey', 'soc.religion.christian', 'sci.crypt', 'comp.graphics']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis\n",
    "`scikit-learn` also comes with a great and useful dimensionality reduction model called `Truncated Singular Value Decomposition` (`TruncatedSVD`). \n",
    "\n",
    "In the event where `TruncatedSVD` model is fitted with `count` or `tfidf` matrices, it is also known as `Latent Semantic Analysis (LSA)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa = TruncatedSVD(n_components=n_components, random_state=1, algorithm='arpack').fit(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LatentDirichletAllocation (LDA)\n",
    "LDA is a good generative probabilistic model for identifying abstract topics from discrete dataset such as text corpora.\n",
    "\n",
    "\n",
    "You should use CountVectorizer when fitting LDA instead of TfidfVectorizer since LDA is based on term count and document count. Fitting LDA with TfidfVectorizer will result in rare words being dis-proportionally sampled. As a result, they will have greater impact and influence on the final topic distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=n_components, random_state=1).fit(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看学习到的Topics信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_topics(model, vectorizer, topics, n_top_words=n_top_words):\n",
    "    word_dict = {}\n",
    "    feature_names = vectorizer.get_feature_names() # 返回词组列表\n",
    "    #∣V∣为单词个数，∣T∣为SVD降维后的主题个数。\n",
    "    # model.components是一个大小为(∣T∣,∣V∣)的矩阵，\n",
    "    # 每一行为主题在每个单词上的分布。我们可以通过这个矩阵得到哪些词对主题t贡献最大。\n",
    "    # 也就是说topic_idx的取值是5个topics，topic\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        #表示对数据进行从大到小（最后有-1是从大到小，没有是从小到大）进行排序,返回数据的索引值\n",
    "        #-n_top_words - 1：指取前20个index\n",
    "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind] # 取词组\n",
    "        word_dict[topics[topic_idx]] = top_features #存成字典的形式\n",
    "\n",
    "    return pd.DataFrame(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>soc.religion.christian</th>\n",
       "      <th>sci.crypt</th>\n",
       "      <th>rec.sport.hockey</th>\n",
       "      <th>talk.politics.mideast</th>\n",
       "      <th>comp.graphics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>edu</td>\n",
       "      <td>key</td>\n",
       "      <td>game</td>\n",
       "      <td>armenian</td>\n",
       "      <td>graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>com</td>\n",
       "      <td>clipper</td>\n",
       "      <td>hockey</td>\n",
       "      <td>israel</td>\n",
       "      <td>thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>writes</td>\n",
       "      <td>chip</td>\n",
       "      <td>team</td>\n",
       "      <td>armenians</td>\n",
       "      <td>image</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>god</td>\n",
       "      <td>encryption</td>\n",
       "      <td>ca</td>\n",
       "      <td>turkish</td>\n",
       "      <td>edu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>people</td>\n",
       "      <td>com</td>\n",
       "      <td>espn</td>\n",
       "      <td>israeli</td>\n",
       "      <td>files</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>article</td>\n",
       "      <td>keys</td>\n",
       "      <td>games</td>\n",
       "      <td>armenia</td>\n",
       "      <td>file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>don</td>\n",
       "      <td>escrow</td>\n",
       "      <td>nhl</td>\n",
       "      <td>jews</td>\n",
       "      <td>program</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>know</td>\n",
       "      <td>clipper chip</td>\n",
       "      <td>season</td>\n",
       "      <td>turks</td>\n",
       "      <td>ac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>just</td>\n",
       "      <td>government</td>\n",
       "      <td>play</td>\n",
       "      <td>turkey</td>\n",
       "      <td>format</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>think</td>\n",
       "      <td>netcom</td>\n",
       "      <td>edu</td>\n",
       "      <td>arab</td>\n",
       "      <td>ftp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>like</td>\n",
       "      <td>netcom com</td>\n",
       "      <td>players</td>\n",
       "      <td>muslim</td>\n",
       "      <td>looking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ca</td>\n",
       "      <td>algorithm</td>\n",
       "      <td>year</td>\n",
       "      <td>government</td>\n",
       "      <td>3d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>does</td>\n",
       "      <td>crypto</td>\n",
       "      <td>wings</td>\n",
       "      <td>soviet</td>\n",
       "      <td>gif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>key</td>\n",
       "      <td>nsa</td>\n",
       "      <td>leafs</td>\n",
       "      <td>genocide</td>\n",
       "      <td>help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>good</td>\n",
       "      <td>phone</td>\n",
       "      <td>buffalo</td>\n",
       "      <td>serdar</td>\n",
       "      <td>uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>clipper</td>\n",
       "      <td>security</td>\n",
       "      <td>fans</td>\n",
       "      <td>argic</td>\n",
       "      <td>images</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>time</td>\n",
       "      <td>use</td>\n",
       "      <td>player</td>\n",
       "      <td>serdar argic</td>\n",
       "      <td>software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>israel</td>\n",
       "      <td>public</td>\n",
       "      <td>win</td>\n",
       "      <td>arabs</td>\n",
       "      <td>windows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>cs</td>\n",
       "      <td>des</td>\n",
       "      <td>cup</td>\n",
       "      <td>war</td>\n",
       "      <td>package</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>say</td>\n",
       "      <td>sternlight</td>\n",
       "      <td>playoff</td>\n",
       "      <td>killed</td>\n",
       "      <td>mail</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   soc.religion.christian     sci.crypt rec.sport.hockey  \\\n",
       "0                     edu           key             game   \n",
       "1                     com       clipper           hockey   \n",
       "2                  writes          chip             team   \n",
       "3                     god    encryption               ca   \n",
       "4                  people           com             espn   \n",
       "5                 article          keys            games   \n",
       "6                     don        escrow              nhl   \n",
       "7                    know  clipper chip           season   \n",
       "8                    just    government             play   \n",
       "9                   think        netcom              edu   \n",
       "10                   like    netcom com          players   \n",
       "11                     ca     algorithm             year   \n",
       "12                   does        crypto            wings   \n",
       "13                    key           nsa            leafs   \n",
       "14                   good         phone          buffalo   \n",
       "15                clipper      security             fans   \n",
       "16                   time           use           player   \n",
       "17                 israel        public              win   \n",
       "18                     cs           des              cup   \n",
       "19                    say    sternlight          playoff   \n",
       "\n",
       "   talk.politics.mideast comp.graphics  \n",
       "0               armenian      graphics  \n",
       "1                 israel        thanks  \n",
       "2              armenians         image  \n",
       "3                turkish           edu  \n",
       "4                israeli         files  \n",
       "5                armenia          file  \n",
       "6                   jews       program  \n",
       "7                  turks            ac  \n",
       "8                 turkey        format  \n",
       "9                   arab           ftp  \n",
       "10                muslim       looking  \n",
       "11            government            3d  \n",
       "12                soviet           gif  \n",
       "13              genocide          help  \n",
       "14                serdar            uk  \n",
       "15                 argic        images  \n",
       "16          serdar argic      software  \n",
       "17                 arabs       windows  \n",
       "18                   war       package  \n",
       "19                killed          mail  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_topics(lsa, tfidf_vectorizer, lsa_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>talk.politics.mideast</th>\n",
       "      <th>rec.sport.hockey</th>\n",
       "      <th>soc.religion.christian</th>\n",
       "      <th>sci.crypt</th>\n",
       "      <th>comp.graphics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>edu</td>\n",
       "      <td>com</td>\n",
       "      <td>edu</td>\n",
       "      <td>edu</td>\n",
       "      <td>armenian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>game</td>\n",
       "      <td>key</td>\n",
       "      <td>image</td>\n",
       "      <td>god</td>\n",
       "      <td>turkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>team</td>\n",
       "      <td>clipper</td>\n",
       "      <td>graphics</td>\n",
       "      <td>people</td>\n",
       "      <td>armenians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hockey</td>\n",
       "      <td>encryption</td>\n",
       "      <td>jpeg</td>\n",
       "      <td>don</td>\n",
       "      <td>people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ca</td>\n",
       "      <td>chip</td>\n",
       "      <td>file</td>\n",
       "      <td>think</td>\n",
       "      <td>jews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>government</td>\n",
       "      <td>available</td>\n",
       "      <td>writes</td>\n",
       "      <td>armenia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>play</td>\n",
       "      <td>writes</td>\n",
       "      <td>mail</td>\n",
       "      <td>know</td>\n",
       "      <td>said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>games</td>\n",
       "      <td>edu</td>\n",
       "      <td>com</td>\n",
       "      <td>just</td>\n",
       "      <td>turkey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>year</td>\n",
       "      <td>db</td>\n",
       "      <td>ftp</td>\n",
       "      <td>like</td>\n",
       "      <td>war</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>use</td>\n",
       "      <td>software</td>\n",
       "      <td>article</td>\n",
       "      <td>government</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nhl</td>\n",
       "      <td>article</td>\n",
       "      <td>use</td>\n",
       "      <td>say</td>\n",
       "      <td>muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>keys</td>\n",
       "      <td>data</td>\n",
       "      <td>does</td>\n",
       "      <td>turks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>season</td>\n",
       "      <td>security</td>\n",
       "      <td>images</td>\n",
       "      <td>way</td>\n",
       "      <td>muslims</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>16</td>\n",
       "      <td>public</td>\n",
       "      <td>program</td>\n",
       "      <td>time</td>\n",
       "      <td>israeli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>people</td>\n",
       "      <td>files</td>\n",
       "      <td>com</td>\n",
       "      <td>israel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>just</td>\n",
       "      <td>bit</td>\n",
       "      <td>did</td>\n",
       "      <td>000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20</td>\n",
       "      <td>des</td>\n",
       "      <td>format</td>\n",
       "      <td>israel</td>\n",
       "      <td>genocide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>win</td>\n",
       "      <td>escrow</td>\n",
       "      <td>computer</td>\n",
       "      <td>believe</td>\n",
       "      <td>soviet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>la</td>\n",
       "      <td>law</td>\n",
       "      <td>version</td>\n",
       "      <td>said</td>\n",
       "      <td>killed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>18</td>\n",
       "      <td>clipper chip</td>\n",
       "      <td>information</td>\n",
       "      <td>jesus</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   talk.politics.mideast rec.sport.hockey soc.religion.christian sci.crypt  \\\n",
       "0                    edu              com                    edu       edu   \n",
       "1                   game              key                  image       god   \n",
       "2                   team          clipper               graphics    people   \n",
       "3                 hockey       encryption                   jpeg       don   \n",
       "4                     ca             chip                   file     think   \n",
       "5                     10       government              available    writes   \n",
       "6                   play           writes                   mail      know   \n",
       "7                  games              edu                    com      just   \n",
       "8                   year               db                    ftp      like   \n",
       "9                     11              use               software   article   \n",
       "10                   nhl          article                    use       say   \n",
       "11                    12             keys                   data      does   \n",
       "12                season         security                 images       way   \n",
       "13                    16           public                program      time   \n",
       "14                    14           people                  files       com   \n",
       "15                    15             just                    bit       did   \n",
       "16                    20              des                 format    israel   \n",
       "17                   win           escrow               computer   believe   \n",
       "18                    la              law                version      said   \n",
       "19                    18     clipper chip            information     jesus   \n",
       "\n",
       "   comp.graphics  \n",
       "0       armenian  \n",
       "1        turkish  \n",
       "2      armenians  \n",
       "3         people  \n",
       "4           jews  \n",
       "5        armenia  \n",
       "6           said  \n",
       "7         turkey  \n",
       "8            war  \n",
       "9     government  \n",
       "10        muslim  \n",
       "11         turks  \n",
       "12       muslims  \n",
       "13       israeli  \n",
       "14        israel  \n",
       "15           000  \n",
       "16      genocide  \n",
       "17        soviet  \n",
       "18        killed  \n",
       "19         world  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_topics(lda, tf_vectorizer, lda_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于学习到的模型进行推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inference(model, vectorizer, topics, text, threshold):\n",
    "    # 这个函数的意思是：输入任意一个文本，给它判断主题\n",
    "    v_text = vectorizer.transform([text]) #embedding\n",
    "    score = model.transform(v_text) #用模型对这个文本打分判断主题\n",
    "    labels = set() \n",
    "    for i in range(len(score[0])):\n",
    "        if score[0][i] > threshold: #设置一个阈值，只有模型评分高于阈值才有机会给它赋予主题\n",
    "            labels.add(topics[i])\n",
    "\n",
    "    if not labels:\n",
    "        return 'None', -1, set()\n",
    "\n",
    "    return topics[np.argmax(score)], score, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.graphics\n",
      "[[ 0.0525437   0.05812372  0.01217637 -0.0393242   0.14331403]]\n"
     ]
    }
   ],
   "source": [
    "text = 'you should use either jpeg or png files for it'\n",
    "topic, score, labels = get_inference(lsa, tfidf_vectorizer, lsa_topics, text, 0)\n",
    "print(topic)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soc.religion.christian\n",
      "[[0.050002   0.05052469 0.79926921 0.05011649 0.05008762]]\n"
     ]
    }
   ],
   "source": [
    "topic, score, labels = get_inference(lda, tf_vectorizer, lda_topics, text, 0)\n",
    "print(topic)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用学习到的特征分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集上其他指标：\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "         comp.graphics       0.90      0.88      0.89       199\n",
      "      rec.sport.hockey       0.96      0.94      0.95       197\n",
      "             sci.crypt       0.92      0.92      0.92       198\n",
      "soc.religion.christian       0.70      0.90      0.79       196\n",
      " talk.politics.mideast       0.92      0.68      0.78       190\n",
      "\n",
      "              accuracy                           0.87       980\n",
      "             macro avg       0.88      0.87      0.87       980\n",
      "          weighted avg       0.88      0.87      0.87       980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda_vec = lda.transform(tf)\n",
    "nb.fit(lda_vec, y_train)\n",
    "\n",
    "lda_test_vec = lda.transform(X_test_tf)\n",
    "y_predict = nb.predict(lda_test_vec)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"测试集上其他指标：\\n\",classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 请针对上述实验结果，分析LDA模型的适用性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
